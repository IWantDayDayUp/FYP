{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efce75bb",
   "metadata": {},
   "source": [
    "# Dataset Generation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f4608b",
   "metadata": {},
   "source": [
    "## Info\n",
    "\n",
    "预训练所需的数据集不需要label, 所以所有数据集都可以使用\n",
    "但, 下游分类任务需要有label的数据集来进行监督训练, 所以只能采用有label的数据集\n",
    "\n",
    "可以成功处理的数据集:\n",
    "\n",
    "- `afdb` - MIT-BIH Atrial Fibrillation Database\n",
    "- `apnea-ecg` - Apnea-ECG Database\n",
    "- `edb` - European ST-T Database\n",
    "- `fantasia` - Fantasia Database\n",
    "- `incartdb` - St Petersburg INCART 12-lead Arrhythmia Database\n",
    "- `ltafdb` - Long Term AF Database\n",
    "- `mitdb` - MIT-BIH Arrhythmia\n",
    "- `nsrdb` - MIT-BIH Normal Sinus Rhythm Database\n",
    "- `ptbdb` - PTB Diagnostic ECG Database\n",
    "- `qtdb` - QT Database\n",
    "- `sddb` - Sudden Cardiac Death Holter Database\n",
    "\n",
    "等待处理的数据集:\n",
    "\n",
    "- `ptb-xl` - (1.7G)PTB-XL, a large publicly available\n",
    "- `chapman` - (2.3G)A large scale 12-lead electrocardiogram database for arrhythmia study\n",
    "\n",
    "以下数据集分别包含多个数据集, 处理起来比较麻烦, 暂时不考虑\n",
    "\n",
    "- `challenge-2017` - (1.4G)AF Classification from a Short Single Lead ECG Recording: The PhysioNet/Computing in Cardiology Challenge 2017\n",
    "- `challenge-2020` - (不确定大小)Classification of 12-lead ECGs: The PhysioNet/Computing in Cardiology Challenge 2020\n",
    "- `chfdb` - (580.5MB)BIDMC Congestive Heart Failure Database\n",
    "- `challenge-2022` - (1.3GB, 但包含多个数据集)Heart Murmur Detection from Phonocardiogram Recordings: The George B. Moody PhysioNet Challenge 2022\n",
    "\n",
    "以下数据集太大, 暂时不考虑\n",
    "\n",
    "- `ltstdb` - (9.5GB)Long Term ST Database\n",
    "- `mimic3wdb` - (6.7TB)MIMIC-III Waveform Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b4d1a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4cc20c",
   "metadata": {},
   "source": [
    "## Part 1: Pretraining Dataset Builder (v1, single-lead)\n",
    "\n",
    "### 1.1 Setup & Global Config\n",
    "\n",
    "Goal of this notebook:\n",
    "\n",
    "1. For multiple PhysioNet ECG databases, build a unified pretraining dataset:\n",
    "   - Single-lead only (for now)\n",
    "   - Unified sampling rate (TARGET_FS)\n",
    "   - Fixed window length (WINDOW_SEC)\n",
    "   - Per-window z-score normalization\n",
    "2. For each database, save sharded `.npy` files to avoid huge files.\n",
    "3. Optionally merge all databases into a global pretraining dataset (also sharded).\n",
    "\n",
    "Later we will extend from single-lead to 12-lead, but v1 focuses on single-lead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "570d7eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wfdb\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from scipy.signal import resample_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4dcb40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET_FS: 500\n",
      "WINDOW_SEC: 10\n",
      "OUT_ROOT: ..\\data\\pretrain\\pretrain_singlelead_500hz_10s\n",
      "Ready DBs: ['afdb', 'apnea', 'edb', 'fantasia', 'incartdb', 'ltafdb', 'mitdb', 'nsrdb', 'ptbdb', 'qtdb', 'sddb']\n",
      "TODO DBs: ['chapman']\n"
     ]
    }
   ],
   "source": [
    "# --------------------\n",
    "# Global config\n",
    "# --------------------\n",
    "TARGET_FS = 500  # target sampling rate in Hz\n",
    "WINDOW_SEC = 10  # window length in seconds\n",
    "WINDOW_STRIDE_SEC = 10  # stride for pretraining windows (no overlap for now)\n",
    "\n",
    "# Each shard will contain at most this many windows\n",
    "MAX_WINDOWS_PER_SHARD = 100_000\n",
    "\n",
    "# Root paths (adapt these to your own environment)\n",
    "DATA_ROOT = Path(r\"../data\")\n",
    "OUT_ROOT = DATA_ROOT / f\"pretrain/pretrain_singlelead_{TARGET_FS}hz_{WINDOW_SEC}s\"\n",
    "\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Databases that are already confirmed to be readable\n",
    "DBS = {\n",
    "    \"afdb\": DATA_ROOT / \"afdb\",\n",
    "    \"apnea\": DATA_ROOT / \"apnea\",\n",
    "    \"edb\": DATA_ROOT / \"edb\",\n",
    "    \"fantasia\": DATA_ROOT / \"fantasia\",\n",
    "    \"incartdb\": DATA_ROOT / \"incartdb\",\n",
    "    \"ltafdb\": DATA_ROOT / \"ltafdb\",\n",
    "    \"mitdb\": DATA_ROOT / \"mitdb\",\n",
    "    \"nsrdb\": DATA_ROOT / \"nsrdb\",\n",
    "    \"ptbdb\": DATA_ROOT / \"ptbdb\",\n",
    "    \"qtdb\": DATA_ROOT / \"qtdb\",\n",
    "    \"sddb\": DATA_ROOT / \"sddb\",\n",
    "    # \"ptb-xl\": DATA_ROOT / \"ptb-xl\",\n",
    "    # \"chapman\": DATA_ROOT / \"chapman\",\n",
    "}\n",
    "\n",
    "# Databases you plan to handle later (non-WFDB, CSV-based, etc.)\n",
    "DBS_TODO = {\n",
    "    # \"ptb-xl\": DATA_ROOT / \"ptb-xl\",\n",
    "    \"chapman\": DATA_ROOT\n",
    "    / \"chapman\",\n",
    "}\n",
    "\n",
    "print(\"TARGET_FS:\", TARGET_FS)\n",
    "print(\"WINDOW_SEC:\", WINDOW_SEC)\n",
    "print(\"OUT_ROOT:\", OUT_ROOT)\n",
    "print(\"Ready DBs:\", list(DBS.keys()))\n",
    "print(\"TODO DBs:\", list(DBS_TODO.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa004876",
   "metadata": {},
   "source": [
    "### Inspect lead names for each database\n",
    "\n",
    "Different PhysioNet databases use different lead configurations and names:\n",
    "- Some are [\"MLII\", \"V5\"]\n",
    "- Some are [\"I\", \"II\", \"III\", \"aVR\", \"aVL\", \"aVF\", \"V1\", ...]\n",
    "- Some long-term Holter recordings only have 1–2 channels.\n",
    "\n",
    "We first implement a helper function to:\n",
    "- Scan a few `.hea` files for each DB\n",
    "- Print:\n",
    "  - record name\n",
    "  - number of signals\n",
    "  - signal names (`sig_name`)\n",
    "\n",
    "This helps us design a robust `pick_single_lead(db_name, rec)` function later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "859ab25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Inspecting DB: afdb ===\n",
      "Total .hea files: 25\n",
      "  [WARN] Failed to read ..\\data\\afdb\\1.0.0\\00735: sampto must be greater than sampfrom\n",
      "  [WARN] Failed to read ..\\data\\afdb\\1.0.0\\03665: sampto must be greater than sampfrom\n",
      "  Record: 04015.hea\n",
      "    n_sig: 2\n",
      "    sig_name: ['ECG1', 'ECG2']\n",
      "\n",
      "=== Inspecting DB: apnea ===\n",
      "Total .hea files: 86\n",
      "  Record: a01.hea\n",
      "    n_sig: 1\n",
      "    sig_name: ['ECG']\n",
      "  [WARN] Failed to read ..\\data\\apnea\\a01er: Samples were not loaded correctly\n",
      "  Record: a01r.hea\n",
      "    n_sig: 4\n",
      "    sig_name: ['Resp C', 'Resp A', 'Resp N', 'SpO2']\n",
      "\n",
      "=== Inspecting DB: edb ===\n",
      "Total .hea files: 90\n",
      "  Record: e0103.hea\n",
      "    n_sig: 2\n",
      "    sig_name: ['V4', 'MLIII']\n",
      "  Record: e0104.hea\n",
      "    n_sig: 2\n",
      "    sig_name: ['MLIII', 'V4']\n",
      "  Record: e0105.hea\n",
      "    n_sig: 2\n",
      "    sig_name: ['MLIII', 'V4']\n",
      "\n",
      "=== Inspecting DB: fantasia ===\n",
      "Total .hea files: 40\n",
      "  Record: f1o01.hea\n",
      "    n_sig: 2\n",
      "    sig_name: ['RESP', 'ECG']\n",
      "  Record: f1o02.hea\n",
      "    n_sig: 2\n",
      "    sig_name: ['RESP', 'ECG']\n",
      "  Record: f1o03.hea\n",
      "    n_sig: 2\n",
      "    sig_name: ['RESP', 'ECG']\n",
      "\n",
      "=== Inspecting DB: incartdb ===\n",
      "Total .hea files: 75\n",
      "  Record: I01.hea\n",
      "    n_sig: 12\n",
      "    sig_name: ['I', 'II', 'III', 'AVR', 'AVL', 'AVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
      "  Record: I02.hea\n",
      "    n_sig: 12\n",
      "    sig_name: ['I', 'II', 'III', 'AVR', 'AVL', 'AVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
      "  Record: I03.hea\n",
      "    n_sig: 12\n",
      "    sig_name: ['I', 'II', 'III', 'AVR', 'AVL', 'AVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
      "\n",
      "=== Inspecting DB: ltafdb ===\n",
      "Total .hea files: 84\n",
      "  Record: 00.hea\n",
      "    n_sig: 2\n",
      "    sig_name: ['ECG', 'ECG']\n",
      "  Record: 01.hea\n",
      "    n_sig: 2\n",
      "    sig_name: ['ECG', 'ECG']\n",
      "  Record: 03.hea\n",
      "    n_sig: 2\n",
      "    sig_name: ['ECG', 'ECG']\n",
      "\n",
      "=== Inspecting DB: mitdb ===\n",
      "Total .hea files: 71\n",
      "  Record: 100.hea\n",
      "    n_sig: 2\n",
      "    sig_name: ['MLII', 'V5']\n",
      "  Record: 101.hea\n",
      "    n_sig: 2\n",
      "    sig_name: ['MLII', 'V1']\n",
      "  Record: 102.hea\n",
      "    n_sig: 2\n",
      "    sig_name: ['V5', 'V2']\n",
      "\n",
      "=== Inspecting DB: nsrdb ===\n",
      "Total .hea files: 18\n",
      "  Record: 16265.hea\n",
      "    n_sig: 2\n",
      "    sig_name: ['ECG1', 'ECG2']\n",
      "  Record: 16272.hea\n",
      "    n_sig: 2\n",
      "    sig_name: ['ECG1', 'ECG2']\n",
      "  Record: 16273.hea\n",
      "    n_sig: 2\n",
      "    sig_name: ['ECG1', 'ECG2']\n",
      "\n",
      "=== Inspecting DB: ptbdb ===\n",
      "Total .hea files: 549\n",
      "  Record: s0010_re.hea\n",
      "    n_sig: 15\n",
      "    sig_name: ['i', 'ii', 'iii', 'avr', 'avl', 'avf', 'v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'vx', 'vy', 'vz']\n",
      "  Record: s0014lre.hea\n",
      "    n_sig: 15\n",
      "    sig_name: ['i', 'ii', 'iii', 'avr', 'avl', 'avf', 'v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'vx', 'vy', 'vz']\n",
      "  Record: s0016lre.hea\n",
      "    n_sig: 15\n",
      "    sig_name: ['i', 'ii', 'iii', 'avr', 'avl', 'avf', 'v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'vx', 'vy', 'vz']\n",
      "\n",
      "=== Inspecting DB: qtdb ===\n",
      "Total .hea files: 105\n",
      "  Record: sel100.hea\n",
      "    n_sig: 2\n",
      "    sig_name: ['MLII', 'V5']\n",
      "  Record: sel102.hea\n",
      "    n_sig: 2\n",
      "    sig_name: ['V5', 'V2']\n",
      "  Record: sel103.hea\n",
      "    n_sig: 2\n",
      "    sig_name: ['MLII', 'V2']\n",
      "\n",
      "=== Inspecting DB: sddb ===\n",
      "Total .hea files: 23\n",
      "  Record: 30.hea\n",
      "    n_sig: 2\n",
      "    sig_name: ['ECG', 'ECG']\n",
      "  Record: 31.hea\n",
      "    n_sig: 2\n",
      "    sig_name: ['ECG', 'ECG']\n",
      "  Record: 32.hea\n",
      "    n_sig: 2\n",
      "    sig_name: ['ECG', 'ECG']\n"
     ]
    }
   ],
   "source": [
    "def inspect_leads_for_db(db_name: str, db_dir: Path, max_records: int = 5):\n",
    "    \"\"\"\n",
    "    Inspect a few records from a PhysioNet-style database and print lead info.\n",
    "    \"\"\"\n",
    "    hea_files = sorted(db_dir.glob(\"**/*.hea\"))\n",
    "    if not hea_files:\n",
    "        print(f\"[{db_name}] No .hea files found in {db_dir}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n=== Inspecting DB: {db_name} ===\")\n",
    "    print(\"Total .hea files:\", len(hea_files))\n",
    "\n",
    "    for hea in hea_files[:max_records]:\n",
    "        rec_name = str(hea.with_suffix(\"\"))\n",
    "        try:\n",
    "            rec = wfdb.rdrecord(rec_name)\n",
    "        except Exception as e:\n",
    "            print(f\"  [WARN] Failed to read {rec_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        sig_names = getattr(rec, \"sig_name\", None)\n",
    "        n_sig = getattr(rec, \"n_sig\", None)\n",
    "\n",
    "        print(f\"  Record: {hea.name}\")\n",
    "        print(f\"    n_sig: {n_sig}\")\n",
    "        print(f\"    sig_name: {sig_names}\")\n",
    "\n",
    "\n",
    "for name, path in DBS.items():\n",
    "    inspect_leads_for_db(name, path, max_records=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2be4a0",
   "metadata": {},
   "source": [
    "### 1.3 Single-lead Selection Strategy\n",
    "\n",
    "We create a `LEAD_PREFERENCE` mapping:\n",
    "\n",
    "- For each database name, specify an ordered list of preferred lead names.\n",
    "- When processing a record:\n",
    "  1. If the record contains any preferred lead, we use the first found.\n",
    "  2. If none of them exist, we fall back to the first channel and print a warning.\n",
    "\n",
    "This function will be used by all later preprocessing steps.\n",
    "\n",
    "Later, when we move to 12-lead, we can replace this with a `pick_multi_lead(db_name, rec)` that returns [T, n_leads]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "357c827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should adjust these preferences after looking at Part 1 output if needed.\n",
    "LEAD_PREFERENCE = {\n",
    "    # Long-term AF / Holter style databases\n",
    "    \"afdb\": [\"ECG1\", \"ECG2\"],\n",
    "    \"ltafdb\": [\"ECG\"],  # both channels named \"ECG\"\n",
    "    \"sddb\": [\"ECG\"],  # both channels named \"ECG\"\n",
    "    \"nsrdb\": [\"ECG1\", \"ECG2\"],\n",
    "    # Apnea-ECG: only records with real ECG channel\n",
    "    # Some records only have respiration/SpO2; we will treat them specially\n",
    "    \"apnea-ecg\": [\"ECG\"],\n",
    "    # ST-T / ischemia related\n",
    "    \"edb\": [\"MLIII\", \"V4\"],\n",
    "    # Fantasia: [RESP, ECG]\n",
    "    \"fantasia\": [\"ECG\"],\n",
    "    # Incart: clean 12-lead, standard names\n",
    "    \"incartdb\": [\"I\", \"II\", \"MLII\"],\n",
    "    # Classic arrhythmia (MIT-BIH)\n",
    "    \"mitdb\": [\"MLII\", \"II\", \"V1\", \"V5\"],\n",
    "    # QT database: very similar style to MIT-BIH\n",
    "    \"qtdb\": [\"MLII\", \"II\", \"V5\", \"V2\"],\n",
    "    # PTB Diagnostic: 15-lead, lowercase names\n",
    "    \"ptbdb\": [\"i\", \"ii\"],\n",
    "}\n",
    "\n",
    "\n",
    "def pick_single_lead(db_name: str, rec: wfdb.Record) -> np.ndarray | None:\n",
    "    \"\"\"\n",
    "    Select a single lead from a multi-lead ECG record.\n",
    "\n",
    "    Returns:\n",
    "        1D signal of shape [T], or None if no suitable lead is found.\n",
    "    \"\"\"\n",
    "    sig = rec.p_signal  # shape: [T, n_sig] or [T]\n",
    "    sig_names = getattr(rec, \"sig_name\", None)\n",
    "    n_sig = getattr(rec, \"n_sig\", sig.shape[1] if sig.ndim == 2 else 1)\n",
    "\n",
    "    # Single-channel case: just return it\n",
    "    if sig.ndim == 1 or n_sig == 1:\n",
    "        return sig.astype(np.float32).reshape(-1)\n",
    "\n",
    "    # Multi-channel case: try preference list\n",
    "    prefs = LEAD_PREFERENCE.get(db_name, None)\n",
    "    if prefs is not None and sig_names is not None:\n",
    "        name_to_idx = {name: idx for idx, name in enumerate(sig_names)}\n",
    "        for pref in prefs:\n",
    "            if pref in name_to_idx:\n",
    "                idx = name_to_idx[pref]\n",
    "                return sig[:, idx].astype(np.float32).reshape(-1)\n",
    "\n",
    "    # Special handling: for some DBs we prefer to SKIP records without ECG\n",
    "    # rather than falling back to an arbitrary channel (e.g., apnea-ecg with Resp only).\n",
    "    dbs_strict = {\"apnea-ecg\"}  # add more if needed\n",
    "\n",
    "    if db_name in dbs_strict:\n",
    "        print(\n",
    "            f\"[INFO] {db_name}: no preferred ECG lead found, skipping this record. \"\n",
    "            f\"sig_names={sig_names}\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    # Fallback: use first channel, but warn\n",
    "    print(f\"[WARN] {db_name}: using channel 0 as fallback, sig_names={sig_names}\")\n",
    "    return sig[:, 0].astype(np.float32).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fd613c",
   "metadata": {},
   "source": [
    "### 1.4 Core preprocessing functions\n",
    "\n",
    "We implement three core utilities:\n",
    "\n",
    "1. `resample_signal`: resample a 1D signal from its original sampling rate to `TARGET_FS`.\n",
    "2. `segment_windows`: cut a long 1D signal into fixed-length windows.\n",
    "3. `zscore_windows`: apply per-window z-score normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "087efbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_signal(sig: np.ndarray, fs_orig: int, fs_target: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Resample a 1D signal from fs_orig to fs_target using polyphase resampling.\n",
    "    \"\"\"\n",
    "    if fs_orig == fs_target:\n",
    "        return sig.astype(np.float32)\n",
    "\n",
    "    # For simplicity, directly use fs_target, fs_orig as up/down.\n",
    "    up = fs_target\n",
    "    down = fs_orig\n",
    "    sig_res = resample_poly(sig, up, down)\n",
    "    return sig_res.astype(np.float32)\n",
    "\n",
    "\n",
    "def segment_windows(\n",
    "    sig: np.ndarray, fs: int, window_sec: int, stride_sec: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Segment a long 1D signal into fixed-length windows.\n",
    "\n",
    "    Args:\n",
    "        sig: 1D array of shape [T]\n",
    "        fs: sampling rate\n",
    "        window_sec: window length in seconds\n",
    "        stride_sec: stride in seconds\n",
    "\n",
    "    Returns:\n",
    "        windows: array of shape [N, window_len]\n",
    "    \"\"\"\n",
    "    window_len = fs * window_sec\n",
    "    stride = fs * stride_sec\n",
    "\n",
    "    if len(sig) < window_len:\n",
    "        return np.zeros((0, window_len), dtype=np.float32)\n",
    "\n",
    "    windows = []\n",
    "    start = 0\n",
    "    while start + window_len <= len(sig):\n",
    "        win = sig[start : start + window_len]\n",
    "        windows.append(win)\n",
    "        start += stride\n",
    "\n",
    "    if not windows:\n",
    "        return np.zeros((0, window_len), dtype=np.float32)\n",
    "\n",
    "    return np.stack(windows, axis=0)\n",
    "\n",
    "\n",
    "def zscore_windows(windows: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply per-window z-score normalization.\n",
    "\n",
    "    Args:\n",
    "        windows: array of shape [N, L]\n",
    "\n",
    "    Returns:\n",
    "        normalized windows, same shape, float32\n",
    "    \"\"\"\n",
    "    mean = windows.mean(axis=1, keepdims=True)\n",
    "    std = windows.std(axis=1, keepdims=True)\n",
    "    std = np.where(std < eps, 1.0, std)\n",
    "    windows_norm = (windows - mean) / std\n",
    "    return windows_norm.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095ff5f8",
   "metadata": {},
   "source": [
    "### 1.4 Sharded saving for a single database\n",
    "\n",
    "For each database:\n",
    "\n",
    "1. Iterate over all `.hea` files.\n",
    "2. For each record:\n",
    "   - Read the record (`wfdb.rdrecord`).\n",
    "   - Select a single lead (using `pick_single_lead`).\n",
    "   - Resample to `TARGET_FS`.\n",
    "   - Segment into windows (`WINDOW_SEC`, `WINDOW_STRIDE_SEC`).\n",
    "   - Apply per-window z-score.\n",
    "3. Accumulate windows in memory until we reach `MAX_WINDOWS_PER_SHARD`, then:\n",
    "   - Save them as a shard npy file.\n",
    "   - Reset the buffer.\n",
    "4. At the end, save any remaining windows as the last shard.\n",
    "\n",
    "Shard filename format:\n",
    "`{db_name}_singlelead_{TARGET_FS}hz_{WINDOW_SEC}s_shardXXX.npy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34443fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_shard(windows_list, db_name: str, shard_id: int, out_dir: Path):\n",
    "    \"\"\"\n",
    "    Save accumulated windows as a shard npy file.\n",
    "    \"\"\"\n",
    "    if not windows_list:\n",
    "        return None\n",
    "\n",
    "    data = np.concatenate(windows_list, axis=0)\n",
    "    shard_name = (\n",
    "        f\"{db_name}_singlelead_{TARGET_FS}hz_{WINDOW_SEC}s_shard{shard_id:03d}.npy\"\n",
    "    )\n",
    "    out_path = out_dir / shard_name\n",
    "    np.save(out_path, data.astype(np.float32))\n",
    "    print(f\"  Saved shard {shard_id} -> {out_path}, windows={data.shape[0]}\")\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def process_db_to_shards(db_name: str, db_dir: Path, out_dir: Path):\n",
    "    \"\"\"\n",
    "    Process a single PhysioNet-style database into sharded pretraining npy files.\n",
    "    \"\"\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    hea_files = sorted(db_dir.glob(\"**/*.hea\"))\n",
    "    if not hea_files:\n",
    "        print(f\"[{db_name}] No .hea files found at {db_dir}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n=== Processing DB: {db_name} ===\")\n",
    "    print(\"Total .hea files:\", len(hea_files))\n",
    "\n",
    "    shard_id = 0\n",
    "    current_windows = []\n",
    "    current_count = 0\n",
    "\n",
    "    for hea in tqdm(hea_files, desc=f\"{db_name}\"):\n",
    "        rec_name = str(hea.with_suffix(\"\"))\n",
    "        try:\n",
    "            rec = wfdb.rdrecord(rec_name)\n",
    "        except Exception as e:\n",
    "            print(f\"  [WARN] Failed to read {rec_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        fs_orig = int(rec.fs)\n",
    "        sig_1d = pick_single_lead(db_name, rec)\n",
    "\n",
    "        # Skip records without valid ECG lead\n",
    "        if sig_1d is None:\n",
    "            continue\n",
    "\n",
    "        sig_res = resample_signal(sig_1d, fs_orig, TARGET_FS)\n",
    "\n",
    "        wins = segment_windows(\n",
    "            sig_res,\n",
    "            fs=TARGET_FS,\n",
    "            window_sec=WINDOW_SEC,\n",
    "            stride_sec=WINDOW_STRIDE_SEC,\n",
    "        )\n",
    "\n",
    "        if wins.size == 0:\n",
    "            continue\n",
    "\n",
    "        wins = zscore_windows(wins)\n",
    "        current_windows.append(wins)\n",
    "        current_count += wins.shape[0]\n",
    "\n",
    "        if current_count >= MAX_WINDOWS_PER_SHARD:\n",
    "            save_shard(current_windows, db_name, shard_id, out_dir)\n",
    "            shard_id += 1\n",
    "            current_windows = []\n",
    "            current_count = 0\n",
    "\n",
    "    if current_windows:\n",
    "        save_shard(current_windows, db_name, shard_id, out_dir)\n",
    "\n",
    "    print(f\"Finished DB: {db_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3902a9",
   "metadata": {},
   "source": [
    "### 1.5 Run preprocessing for all ready databases\n",
    "\n",
    "Now we call `process_db_to_shards` for each database in `DBS`.\n",
    "\n",
    "You can:\n",
    "- First test on a single DB (e.g., \"mitdb\") by commenting others.\n",
    "- After confirming everything is correct, run all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96aa09d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test on a single DB first (e.g., mitdb)\n",
    "# process_db_to_shards(\"mitdb\", DBS[\"mitdb\"], OUT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dcc6f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for db_name, db_dir in DBS.items():\n",
    "#     process_db_to_shards(db_name, db_dir, OUT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "754505d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for db_name, db_dir in DBS_TODO.items():\n",
    "#     process_db_to_shards(db_name, db_dir, OUT_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc587a6",
   "metadata": {},
   "source": [
    "### 1.6 Merge all databases into global pretraining shards\n",
    "\n",
    "We now:\n",
    "\n",
    "1. List all per-DB shard files in `OUT_ROOT`.\n",
    "2. Iterate over them in sorted order.\n",
    "3. Load each shard, accumulate windows into a buffer.\n",
    "4. When the buffer reaches `MAX_WINDOWS_PER_SHARD`, save a global shard:\n",
    "   `all_singlelead_{TARGET_FS}hz_{WINDOW_SEC}s_shardXXX.npy`\n",
    "5. Save any remaining windows at the end.\n",
    "\n",
    "This step is optional:\n",
    "- If your training pipeline can handle multiple per-DB shards directly, you do not need a global merge.\n",
    "- If you prefer a single “global pretrain dataset”, global shards make it convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0faf8379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_global_pretrain_shards(\n",
    "#     out_root: Path, max_windows_per_shard: int = MAX_WINDOWS_PER_SHARD\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Merge all per-DB shards into global pretraining shards.\n",
    "#     \"\"\"\n",
    "#     shard_files = sorted(\n",
    "#         out_root.glob(f\"*singlelead_{TARGET_FS}hz_{WINDOW_SEC}s_shard*.npy\")\n",
    "#     )\n",
    "#     if not shard_files:\n",
    "#         print(\"No per-DB shard files found in\", out_root)\n",
    "#         return\n",
    "\n",
    "#     print(\"Found per-DB shard files:\", len(shard_files))\n",
    "\n",
    "#     global_shard_id = 0\n",
    "#     buffer = []\n",
    "#     buffer_count = 0\n",
    "\n",
    "#     for path in shard_files:\n",
    "#         print(\"Loading\", path)\n",
    "#         arr = np.load(path)\n",
    "\n",
    "#         buffer.append(arr)\n",
    "#         buffer_count += arr.shape[0]\n",
    "\n",
    "#         if buffer_count >= max_windows_per_shard:\n",
    "#             data = np.concatenate(buffer, axis=0)\n",
    "#             fname = f\"all_singlelead_{TARGET_FS}hz_{WINDOW_SEC}s_shard{global_shard_id:03d}.npy\"\n",
    "#             out_path = out_root / fname\n",
    "#             np.save(out_path, data.astype(np.float32))\n",
    "#             print(\n",
    "#                 f\"  Saved global shard {global_shard_id} -> {out_path}, windows={data.shape[0]}\"\n",
    "#             )\n",
    "\n",
    "#             global_shard_id += 1\n",
    "#             buffer = []\n",
    "#             buffer_count = 0\n",
    "\n",
    "#     # Remaining windows\n",
    "#     if buffer:\n",
    "#         data = np.concatenate(buffer, axis=0)\n",
    "#         fname = (\n",
    "#             f\"all_singlelead_{TARGET_FS}hz_{WINDOW_SEC}s_shard{global_shard_id:03d}.npy\"\n",
    "#         )\n",
    "#         out_path = out_root / fname\n",
    "#         np.save(out_path, data.astype(np.float32))\n",
    "#         print(\n",
    "#             f\"  Saved final global shard {global_shard_id} -> {out_path}, windows={data.shape[0]}\"\n",
    "#         )\n",
    "\n",
    "#     print(\"Done building global pretraining shards.\")\n",
    "\n",
    "\n",
    "# build_global_pretrain_shards(OUT_ROOT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
